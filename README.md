# ğŸ”´ Red Team Knowledge Base

**A no-BS collection of LLM attack techniques, jailbreaks, and adversarial prompts â€” built for security researchers who actually test models.**

[![License: MIT](https://img.shields.io/badge/License-MIT-red.svg)](LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

---

## âš ï¸ Read This First

**This repo exists for one reason: to help people test AI models and make them safer.**

Everything here â€” the prompts, the attack chains, the payloads â€” is meant to be used with permission. You test your own systems, you test systems you've been hired to test, or you use it for academic research. That's it.

**Do not** use this to harass people, generate harmful content for distribution, or bypass safety on systems you don't own. If you find a real vulnerability in a commercial model, practice responsible disclosure â€” report it to the vendor before you tweet about it.

I built this because I was frustrated by how scattered red team knowledge is. There's no single place where a researcher can go and say "give me everything." Research papers bury the actionable stuff under 20 pages of methodology. Blog posts cover one technique and miss the big picture. This repo connects all of it.

If this helps you find a vulnerability that gets patched â€” that's a win for everyone.

**Use responsibly. Test with permission. Make AI safer.**

---

## ğŸ¤· What's in here?

I've organized everything into four buckets:

- **`techniques/`** â€” How the attacks actually work. Not theory â€” real mechanics, real payloads, why they bypass safety.
- **`payloads/`** â€” Copy-paste stuff. Jailbreak prompts, adversarial triggers, multi-turn scripts. Grab and go.
- **`scenarios/`** â€” Full attack simulations. Social engineering, malware gen, disinfo campaigns â€” end-to-end walkthroughs.
- **`research/`**, **`tools/`**, **`defenses/`** â€” Papers, open-source tooling, and how to defend against all of the above.

---

## ğŸ“‚ Repo Structure

```
techniques/
â”œâ”€â”€ jailbreaks.md                      â€” Persona & logic-based (DAN, Actor Frame, Dev Mode)
â”œâ”€â”€ injections.md                      â€” Direct & indirect prompt injection
â”œâ”€â”€ obfuscation.md                     â€” Base64, Unicode, ASCII art, encoding tricks
â”œâ”€â”€ multi_turn_attacks.md              â€” Crescendo, Many-Shot, context shifting
â”œâ”€â”€ adversarial_inputs.md              â€” GCG, AutoDAN, PAIR, TAP (automated attacks)
â”œâ”€â”€ 07-echo-chamber-attack.md          â€” Context poisoning via self-reinforcing loops
â”œâ”€â”€ 08-function-call-jailbreak.md      â€” Exploiting the tool-use alignment gap
â”œâ”€â”€ 09-agentic-attacks.md              â€” MCP poisoning, tool chaining, AgentPoison
â”œâ”€â”€ 10-multimodal-vml-attacks.md       â€” VLM typography injection, adversarial images
â”œâ”€â”€ 11-weak-to-strong-cold-attack.md   â€” Inference-time attacks, COLD-Attack
â”œâ”€â”€ 12-compound-attacks.md             â€” â­ CHAINED ATTACKS (the good stuff)
â”œâ”€â”€ 13-system-prompt-extraction-cookbook.md â€” How to extract any system prompt
â””â”€â”€ 14-context-window-manipulation.md  â€” Token smuggling, attention hijacking

payloads/
â”œâ”€â”€ jailbreak_prompts.txt              â€” DAN 12.0, Mongo, Dev Mode (copy-paste)
â”œâ”€â”€ universal_triggers.txt             â€” GCG suffixes, AutoDAN triggers
â”œâ”€â”€ many_shot_template.txt             â€” 100+ shot template for long-context attacks
â”œâ”€â”€ crescendo_scripts.md               â€” Multi-turn dialogue attack trees
â”œâ”€â”€ advanced_attack_templates.md       â€” Payloads for echo chamber, function call, VLM
â””â”€â”€ attack-templates.md                â€” 45+ categorized templates

scenarios/
â”œâ”€â”€ social_engineering.md              â€” Spear-phishing campaign simulation
â”œâ”€â”€ malware_generation.md              â€” FUD malware generation walkthrough
â””â”€â”€ disinformation.md                  â€” Automated disinfo campaign

research/                              â€” 20 key papers, OWASP LLM Top 10, 50+ repos
tools/                                 â€” 25 open-source red teaming tools reviewed
defenses/                              â€” 6-layer defense architecture
```

---

## ğŸ”¥ Start Here

If you're short on time, read these three files in order:

1. **[`12-compound-attacks.md`](techniques/12-compound-attacks.md)** â€” This is the most valuable file. Single-technique attacks barely work anymore against GPT-4o and Claude. Compound chains that stack techniques are what actually punch through. 7 battle-tested recipes with a decision tree.

2. **[`13-system-prompt-extraction-cookbook.md`](techniques/13-system-prompt-extraction-cookbook.md)** â€” Step one of any engagement. Four tiers of extraction from "just ask nicely" to novel 2025 techniques. Includes a single-shot mega-prompt.

3. **[`08-function-call-jailbreak.md`](techniques/08-function-call-jailbreak.md)** â€” The biggest alignment gap in current models. Tool/function calling mode has way less RLHF coverage than chat mode. 90%+ success rate, with a working Python script you can run right now.

For the full technique catalog, the `techniques/` directory has everything from echo chamber attacks to multimodal VLM exploits.

---

## ğŸ“Š What Actually Works (2025)

Let me be real about effectiveness. Here's what I've seen:

| Technique | GPT-4o | Claude 3.5 | Llama 3 | Notes |
|-----------|--------|-----------|---------|-------|
| DAN/Persona (alone) | ~5% | ~3% | ~25% | Basically dead on frontier models |
| Crescendo | ~40% | ~35% | ~65% | Still decent, needs patience |
| Many-Shot | ~70% | ~60% | ~80% | Requires huge context window |
| GCG Suffix | ~85% | ~45% | ~90% | Best automated single-shot |
| Function Call Exploit | ~92% | ~91% | ~96% | **Best single technique right now** |
| Echo Chamber | ~90% | ~88% | ~95% | Requires multi-turn |
| Compound Chains | ~85% | ~80% | ~92% | **Most reliable overall** |

The takeaway: **single techniques are dying**. The models are trained against them. What works is either (a) exploiting alignment gaps they haven't patched yet (function calling, echo chamber) or (b) stacking multiple techniques into compound chains.

---

## ğŸ› ï¸ Tools I Use

For automated testing, these are the ones worth your time:
- **[garak](https://github.com/leondz/garak)** â€” Best general-purpose LLM vulnerability scanner
- **[PyRIT](https://github.com/Azure/PyRIT)** â€” Microsoft's red teaming framework, good for structured testing
- **[HarmBench](https://github.com/centerforaisafety/HarmBench)** â€” Academic benchmark, useful for comparing across models

Full tool reviews in [`tools/open-source-tools.md`](tools/open-source-tools.md).

---

## ğŸ¤ Contributing

If you've found a technique that works and isn't documented here, open a PR. I especially want:
- **New compound chains** â€” novel combinations that break current models
- **Updated success rates** â€” if something stopped working or started working, let me know
- **Model-specific quirks** â€” every model has weird edge cases
- **Defense bypasses** â€” if you've gotten around a specific guardrail, document it

---

## ğŸ“„ License

MIT â€” do what you want with it, but do it responsibly.

---

*Built by someone who thinks AI should be safe, and that the best way to make it safe is to break it first.*
*Last updated: February 2025*
