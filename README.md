# ğŸ”´ RedTeam Knowledge Base

### The most comprehensive open-source collection of LLM red teaming knowledge on the internet.

> Everything you need to understand, test, and improve AI safety â€” research papers, attack techniques, tools, defenses, real-world case studies, and ready-to-use templates.

[![License: MIT](https://img.shields.io/badge/License-MIT-red.svg)](LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

---

## ğŸ¯ What Is This?

This is a structured knowledge base containing **every known technique** for testing LLM safety, organized from 20+ research papers, 25+ open-source tools, 600K+ adversarial prompts, and real-world incident reports. It's designed for:

- **Security Researchers** â€” Understand the full attack landscape
- **AI Engineers** â€” Know what you're defending against
- **Red Teamers** â€” Systematic approach to LLM vulnerability testing
- **AI Companies** â€” Benchmark your model's safety posture

---

## ğŸ“‚ Repository Structure

```
redteam-knowledge-base/
â”‚
â”œâ”€â”€ README.md                          â† You are here
â”‚
â”œâ”€â”€ techniques/                        â† Attack technique encyclopedia
â”‚   â”œâ”€â”€ 01-jailbreak-attacks.md        â† 30+ jailbreak techniques (DAN, Crescendo, Many-Shot, etc.)
â”‚   â”œâ”€â”€ 02-prompt-injection.md         â† Direct & indirect injection methods
â”‚   â”œâ”€â”€ 03-encoding-obfuscation.md     â† Base64, Unicode, ASCII art, multi-language attacks
â”‚   â”œâ”€â”€ 04-system-agent-attacks.md     â† System prompt extraction, agent abuse, RAG poisoning
â”‚   â”œâ”€â”€ 05-multimodal-attacks.md       â† Vision, audio, code gen, and emerging attack vectors
â”‚   â””â”€â”€ 06-real-world-case-studies.md  â† 10+ documented real-world incidents
â”‚
â”œâ”€â”€ research/                          â† Academic foundations
â”‚   â”œâ”€â”€ papers.md                      â† 20 key research papers with summaries & links
â”‚   â”œâ”€â”€ owasp-llm-top10.md           â† Complete OWASP Top 10 for LLM Applications deep dive
â”‚   â””â”€â”€ github-repos.md              â† 50+ GitHub repos, datasets, and platforms
â”‚
â”œâ”€â”€ tools/                             â† Open source tools
â”‚   â””â”€â”€ open-source-tools.md          â† 25 tools: garak, PyRIT, HarmBench, Giskard, and more
â”‚
â”œâ”€â”€ payloads/                          â† Ready-to-use templates
â”‚   â””â”€â”€ attack-templates.md           â† 45+ categorized attack templates
â”‚
â””â”€â”€ defenses/                          â† Know what you're up against
    â””â”€â”€ mitigation-strategies.md      â† 6-layer defense architecture breakdown
```

---

## ğŸ”¥ What's Inside

### ğŸ“– Techniques (50+ documented)
| Category | Count | Highlights |
|----------|-------|------------|
| Jailbreak Attacks | 25+ | DAN, Crescendo, Many-Shot, Skeleton Key, GCG, PAIR, TAP |
| Prompt Injection | 12+ | Direct, indirect, delimiter escape, web/email/RAG injection |
| Encoding & Obfuscation | 10+ | Base64, Unicode smuggling, ASCII art, multi-language |
| System & Agent Attacks | 10+ | Prompt extraction, tool abuse, data exfiltration, DoS |
| Multimodal Attacks | 8+ | Image injection, audio attacks, code gen exploits |
| Real-World Cases | 10+ | Bing Chat, Samsung, DPD, Chevrolet, Air Canada |

### ğŸ“š Research Coverage
- **20 key papers** â€” from GCG (2023) to Many-Shot Jailbreaking (2024)
- **10 datasets** â€” 600K+ HackAPrompt prompts, 38K Anthropic attacks, HarmBench, AdvBench
- **Complete OWASP LLM Top 10** deep dive with attack examples and mitigations

### ğŸ› ï¸ Tools & Frameworks
- **25 open-source tools** reviewed and categorized
- Tier 1 (Production): garak, PyRIT, HarmBench, Giskard, JailbreakBench
- Tier 2 (Specialized): GCG, PAIR, TAP, AutoDAN, HackAPrompt
- Tier 3 (Datasets): Anthropic HH-RLHF, AdvBench, ToxicChat, Do-Not-Answer
- Tier 4 (Defense): LLM Guard, NeMo Guardrails, Rebuff, Vigil

### ğŸ¯ Attack Templates
- **45+ ready-to-use templates** for authorized security testing
- Organized by category: extraction, jailbreak, injection, multi-turn, automated
- Model-specific quirks for GPT-4, Claude, Llama, Gemini

### ğŸ›¡ï¸ Defense Strategies
- **6-layer defense architecture** â€” input filtering â†’ prompt hardening â†’ model-level â†’ output filtering â†’ architectural â†’ monitoring
- Effectiveness matrix: which defenses work against which attacks
- Known bypass techniques for each defense

---

## ğŸš€ Quick Start

### For Security Testing
1. Start with [`techniques/01-jailbreak-attacks.md`](techniques/01-jailbreak-attacks.md) for the attack landscape
2. Pick relevant [`payloads/attack-templates.md`](payloads/attack-templates.md) templates
3. Use tools from [`tools/open-source-tools.md`](tools/open-source-tools.md) for automation

### For Building Defenses
1. Study [`research/owasp-llm-top10.md`](research/owasp-llm-top10.md) for vulnerability categories
2. Implement layers from [`defenses/mitigation-strategies.md`](defenses/mitigation-strategies.md)
3. Test your defenses using the attack templates

### For Research
1. Start with [`research/papers.md`](research/papers.md) for academic foundations
2. Explore datasets linked in [`research/github-repos.md`](research/github-repos.md)
3. Benchmark using HarmBench or JailbreakBench frameworks

---

## ğŸ“Š Attack Success Rate Overview

| Technique | GPT-4 | Claude 3 | Llama 3 | Gemini |
|-----------|-------|----------|---------|--------|
| DAN/Persona | ~5% | ~3% | ~25% | ~8% |
| Crescendo | ~40% | ~35% | ~65% | ~45% |
| Many-Shot | ~70% | ~60% | ~80% | ~65% |
| GCG Suffix | ~85% | ~45% | ~90% | ~70% |
| PAIR/TAP | ~80% | ~60% | ~85% | ~75% |
| Task Decomposition | ~70% | ~55% | ~80% | ~65% |

*Approximate rates from research papers. Actual rates vary by model version and specific prompt.*

---

## âš ï¸ Responsible Use

This knowledge base is for **legitimate security research and defensive purposes**:

- âœ… Testing your own AI systems and applications
- âœ… Academic research on AI safety
- âœ… Improving defenses and guardrails
- âœ… Understanding the threat landscape
- âœ… Security audits with proper authorization
- âŒ Attacking systems you don't own
- âŒ Generating harmful content for malicious purposes
- âŒ Bypassing safety measures for illegal activities

**If you discover vulnerabilities in commercial AI systems, practice responsible disclosure.**

---

## ğŸ¤ Contributing

We welcome contributions! To add new techniques, papers, tools, or case studies:

1. Fork the repository
2. Create a feature branch
3. Add your content following the existing structure
4. Submit a pull request with a clear description

Priority areas for contributions:
- New attack techniques with evidence
- Updated success rates and model-specific data
- Additional real-world case studies
- New open-source tools
- Defense bypass techniques
- Multi-modal attack research

---

## ğŸ“„ License

MIT License â€” Use freely for security research and education.

---

## ğŸŒŸ Star History

If this helped you, give it a â­ â€” it helps others find this resource.

---

*Built with ğŸ”´ by security researchers, for security researchers.*
*Last Updated: February 2026*
