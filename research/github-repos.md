# üêô GitHub Repositories & Resources

> Every significant GitHub repository related to LLM red teaming, jailbreaking, prompt injection, adversarial attacks, and AI safety testing.

---

## ‚öîÔ∏è Attack Tools & Frameworks

| Repository | Stars | Description | Link |
|-----------|-------|-------------|------|
| **NVIDIA/garak** | 3K+ | LLM vulnerability scanner ‚Äî the "nmap for LLMs" | https://github.com/NVIDIA/garak |
| **Azure/PyRIT** | 2K+ | Microsoft's Python Risk Identification Toolkit for GenAI | https://github.com/Azure/PyRIT |
| **llm-attacks** | 3K+ | GCG universal adversarial attack (Zou et al.) | https://github.com/llm-attacks/llm-attacks |
| **patrickchao/jailbreaking-llms** | 500+ | PAIR automated jailbreak implementation | https://github.com/patrickchao/jailbreaking-llms |
| **RICommunity/TAP** | 300+ | Tree of Attacks with Pruning | https://github.com/RICommunity/TAP |
| **SheltonLiu-N/AutoDAN** | 400+ | Readable adversarial prompt generation | https://github.com/SheltonLiu-N/AutoDAN |
| **PromptLabs/hackaprompt** | 300+ | HackAPrompt competition platform | https://github.com/PromptLabs/hackaprompt |
| **utkusen/promptmap** | 400+ | Automated prompt injection testing | https://github.com/utkusen/promptmap |
| **Giskard-AI/giskard** | 3K+ | ML testing including 50+ LLM adversarial probes | https://github.com/Giskard-AI/giskard |

## üìä Benchmarks & Evaluation

| Repository | Description | Link |
|-----------|-------------|------|
| **centerforaisafety/HarmBench** | 18 attack methods √ó 33 LLMs benchmark | https://github.com/centerforaisafety/HarmBench |
| **JailbreakBench/jailbreakbench** | Standardized jailbreak evaluation pipeline | https://github.com/JailbreakBench/jailbreakbench |
| **thu-coai/SafetyBench** | Multi-category safety evaluation | https://github.com/thu-coai/SafetyBench |
| **Libr-AI/do-not-answer** | 939 instructions models should refuse | https://github.com/Libr-AI/do-not-answer |
| **bertiev/SimpleSafetyTests** | 100 simple safety test prompts | https://github.com/bertiev/SimpleSafetyTests |
| **microsoft/promptbench** | Adversarial robustness benchmark | https://github.com/microsoft/promptbench |

## üìö Datasets

| Dataset | Size | Platform | Link |
|---------|------|----------|------|
| **HackAPrompt Dataset** | 600K+ prompts | HuggingFace | https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset |
| **Anthropic HH-RLHF** | 38,961 attacks | GitHub | https://github.com/anthropics/hh-rlhf |
| **ToxicChat** | 10K+ conversations | HuggingFace | https://huggingface.co/datasets/lmsys/toxic-chat |
| **CategoricalHarmfulQA** | Categorized harmful Qs | HuggingFace | https://huggingface.co/datasets/declare-lab/CategoricalHarmfulQA |
| **AdvBench** | 500 harmful strings | GitHub (in llm-attacks) | https://github.com/llm-attacks/llm-attacks |
| **RED-EVAL Dataset** | Multi-turn attacks | HuggingFace | https://huggingface.co/datasets/Jayko/RED-EVAL |

## üõ°Ô∏è Defense & Guardrail Tools

| Repository | Description | Link |
|-----------|-------------|------|
| **protectai/llm-guard** | Input/output guardrails for LLM apps | https://github.com/protectai/llm-guard |
| **NVIDIA/NeMo-Guardrails** | Programmable LLM guardrails | https://github.com/NVIDIA/NeMo-Guardrails |
| **guardrails-ai/guardrails** | Validation framework for LLM outputs | https://github.com/guardrails-ai/guardrails |
| **deadbits/vigil-llm** | Prompt injection detection | https://github.com/deadbits/vigil-llm |
| **protectai/rebuff** | Self-hardening prompt injection detector | https://github.com/protectai/rebuff |
| **whylabs/langkit** | LLM monitoring toolkit | https://github.com/whylabs/langkit |
| **laiyer-ai/llm-guard** | Comprehensive LLM security toolkit | https://github.com/laiyer-ai/llm-guard |

## üìñ Curated Lists & Knowledge Bases

| Repository | Description | Link |
|-----------|-------------|------|
| **corca-ai/awesome-llm-security** | Curated list of LLM security resources | https://github.com/corca-ai/awesome-llm-security |
| **jiren214/awesome-llm-safety** | LLM safety research collection | https://github.com/jiren214/awesome-llm-safety |
| **0xk1h0/ChatGPT_DAN** | Collection of DAN and jailbreak prompts | https://github.com/0xk1h0/ChatGPT_DAN |
| **verazuo/jailbreak_llms** | Jailbreak prompts collection | https://github.com/verazuo/jailbreak_llms |
| **dair-ai/Prompt-Engineering-Guide** | Comprehensive prompt engineering guide | https://github.com/dair-ai/Prompt-Engineering-Guide |
| **brexhq/prompt-engineering** | Prompt engineering tips & tricks | https://github.com/brexhq/prompt-engineering |

## üî¨ Research Implementations

| Repository | Paper | Link |
|-----------|-------|------|
| **llm-attacks** | "Universal Adversarial Attacks" (GCG) | https://github.com/llm-attacks/llm-attacks |
| **patrickchao/jailbreaking-llms** | PAIR Attack | https://github.com/patrickchao/jailbreaking-llms |
| **RICommunity/TAP** | Tree of Attacks with Pruning | https://github.com/RICommunity/TAP |
| **SheltonLiu-N/AutoDAN** | AutoDAN Genetic Jailbreaks | https://github.com/SheltonLiu-N/AutoDAN |
| **PromptLabs/hackaprompt** | HackAPrompt Competition | https://github.com/PromptLabs/hackaprompt |
| **centerforaisafety/HarmBench** | HarmBench Framework | https://github.com/centerforaisafety/HarmBench |
| **anthropics/hh-rlhf** | Anthropic Red Team Data | https://github.com/anthropics/hh-rlhf |

## üåê Useful Websites & Platforms

| Platform | Description | Link |
|----------|-------------|------|
| **HackAPrompt Playground** | Interactive prompt hacking testing | https://huggingface.co/spaces/hackaprompt/playground |
| **OWASP LLM Top 10** | Industry standard vulnerabilities | https://owasp.org/www-project-top-10-for-large-language-model-applications/ |
| **HarmBench Website** | Benchmark results & leaderboards | https://www.harmbench.org |
| **JailbreakBench** | Jailbreak leaderboard | https://jailbreakbench.github.io |
| **MITRE ATLAS** | Adversarial ML threat framework | https://atlas.mitre.org |
| **AI Incident Database** | Real-world AI failure incidents | https://incidentdatabase.ai |
| **LLM Security Blog (Giskard)** | 50+ attack guides | https://www.giskard.ai/knowledge/llm-security |
| **Anthropic Research** | Safety research publications | https://www.anthropic.com/research |

## üéì Learning Resources

| Resource | Type | Link |
|----------|------|------|
| **OffSec LLM Red Teaming** | Course (34 hrs) | https://offsec.com/learning/paths/llm-red-teaming |
| **HuggingFace Red Teaming Guide** | Blog Post | https://huggingface.co/blog/red-teaming |
| **Gandalf by Lakera** | Interactive CTF | https://gandalf.lakera.ai |
| **Tensor Trust** | Prompt injection game | https://tensortrust.ai |
| **Prompt Airlines** | Prompt injection CTF | https://promptairlines.com |

---

*Last Updated: February 2026*
